---
id: rook-ceph
title: Rook Ceph
sidebar_label: Rook Ceph
description: Rook Ceph
keywords:
  - Rook
  - Ceph
---

import useBaseUrl from "@docusaurus/useBaseUrl";

## Rook

<center>
	<img src={useBaseUrl("img/mlops/mlops/storage/ceph/rook-ceph.png")} />
	<strong>
		https://rook.io/docs/rook/latest-release/Getting-Started/storage-architecture/#architecture
	</strong>
</center>

## 설치

```shell
helm repo add rook-release https://charts.rook.io/release
```

### Operator

- https://rook.io/docs/rook/latest-release/Helm-Charts/operator-chart/

```shell
helm repo update rook-release \
&& helm search repo rook-release/rook-ceph -l | head -n 10
```

```shell
helm pull rook-release/rook-ceph --version v1.13.7
```

```shell
helm show values rook-ceph-v1.13.7.tgz > rook-ceph-v1.13.7.yaml
```

```yaml title="rook-ceph-values.yaml"
crds:
  enabled: true

resources:
  requests:
    cpu: 200m
    memory: 128Mi
  limits:
    cpu: "1"
    memory: 512Mi

tolerations: []

csi:
  enableRbdDriver: false
  enableCephfsDriver: true

  # csi-detect-version-
  provisionerTolerations: []

  kubeletDirPath: /var/lib/kubelet
```

```shell
helm template rook-ceph rook-ceph-v1.13.7.tgz \
    -n rook-ceph \
    -f rook-ceph-values.yaml \
    > rook-ceph.yaml
```

```shell
helm upgrade rook-ceph rook-ceph-v1.13.7.tgz \
    --install \
    --history-max 5 \
    -n rook-ceph \
    -f rook-ceph-values.yaml
```

### Cluster

- [Disk 준비](/docs/mlops/mlops/storage/ceph/osd)

Cluster 차트는 일반적으로 생성하는 클러스터의 구성에 대한 CR, StorageClass 등을 생성합니다.

```shell
helm repo update rook-release \
&& helm search repo rook-release/rook-ceph-cluster -l | head -n 10
```

```shell
helm pull rook-release/rook-ceph-cluster --version v1.13.7
```

```shell
helm show values rook-ceph-cluster-v1.13.7.tgz > rook-ceph-cluster-v1.13.7.yaml
```

```yaml title="rook-ceph-cluster-values.yaml"
operatorNamespace: rook-ceph

cephClusterSpec:
  mon:
    count: 3

  mgr:
    count: 2

  placement:
    all:
      tolerations:
        - key: "loliot.net/storage"
          operator: "Exists"

  resources: {}

  storage:
    useAllNodes: false
    useAllDevices: false
    node:
      - name: "ip-192-168-0-11"
        devices:
          - name: "/dev/vg1/lv1"

cephBlockPools: []

cephFileSystems: []

cephObjectStores: []
```

```shell
helm template rook-ceph-cluster rook-ceph-cluster-v1.13.7.tgz \
    -n rook-ceph \
    -f rook-ceph-cluster-values.yaml \
    > rook-ceph-cluster.yaml
```

```shell
helm upgrade rook-ceph-cluster rook-ceph-cluster-v1.13.7.tgz \
    --install \
    --history-max 5 \
    -n rook-ceph \
    -f rook-ceph-cluster-values.yaml
```

## 업그레이드

- https://rook.io/docs/rook/latest-release/Upgrade/rook-upgrade

<br />

- rook-ceph을 업그레이드 합니다.
- rook-ceph-cluster를 업그레이드 합니다.

## 삭제

### Cluster

- https://rook.io/docs/rook/latest-release/Getting-Started/ceph-teardown/
- https://rook.io/docs/rook/latest-release/CRDs/Cluster/ceph-cluster-crd/#cleanup-policy

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
spec:
  cleanupPolicy:
    confirmation: yes-really-destroy-data
```

```shell
helm uninstall rook-ceph-cluster -n rook-ceph
```

Cluster 운영에 사용된 Node에 접속하여 `/var/lib/rook` 디렉토리를 정리합니다.

### Operator

```shell
helm uninstall rook-ceph -n rook-ceph
```

```shell
for crd in $(kubectl get crd -o name); do
    case $crd in
        *ceph.rook.io)
            kubectl delete $crd
            ;;
        *objectbucket.io)
            kubectl delete $crd
            ;;
    esac
done
```
