---
id: pg
title: Ceph Placement Group(PG) 관리
sidebar_label: PG 관리
description: Ceph Placement Group(PG) 관리
keywords:
  - ceph
  - pg
---

## Placement Group(PG)

- [Data 흐름](/docs/mlops/storage/ceph/osd#data-흐름)

## PG AutoScale

- https://docs.ceph.com/en/latest/rados/operations/placement-groups/#viewing-pg-scaling-recommendations

```shell
kubectl rook-ceph ceph osd pool set <pool> <option> <value>
```

- `<option>`: `<value>`
  - `pg_autoscale_mode`: `on`|`warn`|`off`
  - `target_size_bytes`: `<targetSize>`
  - `target_size_ratio`: `<targetRatio>`
    - `target_size_bytes`보다 우선 순위가 높으므로, 둘 다 선언하면 `target_size_bytes`는 무시됩니다.
  - `bulk`: `<bool>`
    - `true`: 규모가 클 것으로 예상되는 pool이라는 설정입니다.

```shell
kubectl rook-ceph ceph osd pool autoscale-status
```

- `POOL`
- `SIZE`: 사용자가 저장한 데이터 크기입니다.
- `TARGET SIZE`
  - 관리자가 예상하는 `SIZE`입니다.
  - autoscaler는 max(targetSize, `실제 사용자가 저장한 데이터 크기`) 값을 고려합니다.
- `RATE`
  - `복제본 등을 포함한 데이터 크기`/`사용자가 저장한 데이터 크기` 입니다.
  - 단순히 `사용자가 저장한 데이터` + `복제본 1개`인 경우 2.0이 됩니다.
- `RAW CAPACITY`: CRUSH에 의해 pool이 매핑된 저장 장치의 총 용량입니다.
- `RATIO`: `SIZE` \* `RATE` / `RAW CAPACITY`
- `TARGET RATIO`: 관리자가 예상하는 `RATIO`입니다.
- `EFFECTIVE RATIO`
  - `TARGET SIZE`가 설정된 경우 예상되는 용량을 고려합니다.
  - `TARGET RATIO`값들을 정규화 합니다.
  - Ex) `TARGET RATIO`가 1.0인 pool이 2개가 있다면, 각각의 `EFFECTIVE RATIO`는 0.5가 됩니다.
- `BIAS`: 예상되는 PG_NUM에 곱하
- `PG_NUM`
- `NEW PG_NUM`
- `AUTOSCALE`: `on`|`warning`|`off`
- `BULK`: `True`|`False`
