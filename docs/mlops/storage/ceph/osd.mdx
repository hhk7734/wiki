---
id: osd
title: Ceph OSD 관리
sidebar_label: OSD 관리
description: Ceph OSD 관리
keywords:
  - ceph
  - osd
---

import useBaseUrl from "@docusaurus/useBaseUrl";

## Disk 준비

아래 중 하나에 해당하는 Disk를 준비해야합니다.

- Raw devices (파티션 또는 파일 시스템이 없는 디바이스)
- Raw partitions (파일 시스템이 없는 파티션)
- LVM Logical Volumes (파일 시스템이 없는 논리적 볼륨)
- `Block`모드로 사용가능한 PV(다른 StorageClass에 의해 프로비저닝 됨)

### Raw device를 사용하는 경우

```shell
sudo dd if=/dev/zero of=/dev/<device> bs=1M count=100 oflag=direct,dsync
```

### Logical Volume(LV)를 사용하는 경우

- [Logical Volume Manager(LVM)](/docs/linux/kernel/storage/lvm)

## Data 흐름

<center>
	<img src={useBaseUrl("img/mlops/storage/ceph/data_flow.png")} />
	<br />
	<sub>https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/storage_strategies_guide/overview</sub>
</center>

- Data
  - block device image(RBD), object(RGW), file(CephFS)
  - Data는 저장을 위한 Object로 분할되고(e.g. 4 MB), Object는 stripe unit으로 분할됩니다(e.g. 64 KB).
  - [Data Striping](https://docs.ceph.com/en/reef/architecture/#data-striping)
- Pool
  - 클라이언트가 인식하는 추상화된 저장공간입니다.
  - Pool은 Placement Group(PG)들로 나뉩니다.
  - 하나의 PG는 하나의 Pool에만 속합니다.
- CRUSH 알고리즘
  - PG를 OSD에 할당합니다.(primary OSD와 secondary OSDs로 구성)
  - Object를 PG 중 하나에 할당합니다.
- CRUSH 맵
  - Bucket 계층 구조, OSD 리스트, Pool 내에서 데이터를 복제 규칙을 정의한 맵입니다.
  - OSD가 추가되면 `host` Bucket에 자동으로 추가됩니다.
  - 기본적으로 `root`-`host`-`osd` 구조가 주어집니다.
  - `root`-`region`-`zone`-`datacenter`-`room`-`pod`-`pdu`-`row`-`rack`-`chassis`-`host`-`osd` 구조로 확장 가능합니다.

```shell
kubectl rook-ceph ceph osd tree
```

## OSD 추가하기

:::info[Reference]

- [Rook / CephCluster CRD](https://rook.io/docs/rook/latest/CRDs/Cluster/ceph-cluster-crd)
- [Rook / Speicification # StorageScopeSpec](https://rook.io/docs/rook/latest/CRDs/specification/#ceph.rook.io/v1.StorageScopeSpec)

:::

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
spec:
  resources:
    osd: {}
  placement:
    all: {}
    osd: {}
  storage:
    useAllNodes: false
    useAllDevices: false
```

:::warning

동일한 설정이 있는 경우 범위가 좁은 것이 적용됩니다. 예를 들어 `storage.config`와 `storage.nodes.config`가 있고 해당 장치가 속한 Node에 config가 있는 경우 `storage.nodes.config`가 적용됩니다.

:::

- `resources`
  - `osd: {}`
  - `osd-<deviceClass>: {}`
    - `<deviceClass>`에는 `hdd`, `ssd`, `nvme` 등이 올 수 있습니다.
  - 메모리 설정은 자동으로 `osd_memory_target` 설정에 적용됩니다.
- `placement`
  - `all`
  - `osd`
    - `nodeAffinity`
    - `podAffinity`
    - `podAntiAffinity`
    - `tolerations`
    - `topologySpreadConstraints`
- `storage`
  - `config: <config>`
  - **deviceSelection**\(임베딩\)
  - `storageClassDeviceSets: []`
  - `onlyApplyOSDPlacement: <bool>`
    - PV가 아닌 장치를 사용하는 경우 `.spec.placement.osd`와 `.spec.placement.all`의 병합 여부입니다.
    - PV 장치를 사용하는 경우 `storageClassDeviceSets`에 정의된 placement와 `.spec.placement.all`의 병합 여부입니다.
  - `useAllNodes: <bool>`: 모든 노드에서 장치를 감지할지 여부입니다.
  - `nodes: []`
    - `name: <nodeName>`
    - `config: <config>`
      - 노드에 있는 모든 장치에 적용될 설정입니다.
    - **deviceSelection**\(임베딩\)
    - `resources: {}`
      - 메모리 설정은 자동으로 `osd_memory_target` 설정에 적용됩니다.

<br />

- **deviceSelection**
  - `useAllDevices: <bool>`
  - `deviceFilter: <deviceRegex>`
    - https://pkg.go.dev/regexp/syntax
  - `devicePathFilter: <devicePathRegex>`
    - https://pkg.go.dev/regexp/syntax
  - `devices: []`
    - `name: <device|devicePath>`
    - `config: <config>`
  - `volumeClaimTemplates: []`

<br />

- `<config>`
  - https://rook.io/docs/rook/latest/CRDs/Cluster/ceph-cluster-crd/#osd-configuration-settings

resources에 memory 설정을 하는 경우 `osd_memory_target` 설정에 반영됩니다. 스토리지만을 위한 Node의 경우 `osd_memory_target_autotune`와 `autotune_memory_target_ratio`을 활용하는 것이 좋습니다.

## OSD 제거하기

:::info[Reference]

- [Rook / Ceph OSD Management # Remove an OSD](https://rook.io/docs/rook/v1.10/Storage-Configuration/Advanced/ceph-osd-mgmt/#remove-an-osd)

:::

```shell
kubectl rook-ceph ceph status
```

1. `cluster.helath`가 **HEALTH_OK**인지 확인합니다.
1. `data.pgs`의 모든 pg가 **active+clean**인지 확인합니다.
1. `data.usage`의 avail 용량이 OSD를 제거한 후 backfilling, rebalancing 등을 고려했을 때 충분한지 확인합니다.

상태확인이 끝난 후, `CephCluster` CR에 device를 직접 선언한 경우 제거하려는 devvice를 CR에서 제거합니다. 한 번에 너무 많은 device를 제거하면 안됩니다.

:::warning

OSD를 제거하는 동안 operator가 제거된 device를 검색해서 다시 추가하려고 시도할 수 있는 경우, 이를 막기 위해 작업이 끝날 때까지 operator의 수를 0으로 설정하여 중지시켜야합니다.

:::

아래 명령어를 통해 OSD를 **down** 상태로 변경합니다.

```shell
kubectl -n rook-ceph scale deploy/rook-ceph-osd-<OSDID> --replicas=0
```

```shell
kubectl rook-ceph ceph osd df osd.<OSDID>
```

```shell
kubectl rook-ceph rook purge-osd <OSDID> --force
```

OSD가 제거된 후 클러스터 상태가 **HEALTH_OK**인지 확인합니다. `failureDomain`이 `host`인 경우 최대 Node 하나, `osd`인 경우 OSD 하나만 제거한 후 모든 작업이 끝날 때까지 기다렸다가 다음 작업을 진행해야합니다.

```shell
kubectl rook-ceph ceph status
```

:::tip

backfilling 속도를 높이기 위해 아래와 같은 설정을 적용할 수 있습니다.

```shell
kubectl rook-ceph ceph config set osd osd_mclock_profile high_recovery_ops
```

```shell
kubectl rook-ceph ceph tell 'osd.*' injectargs '--osd-max-backfills 20'
```

```shell
kubectl rook-ceph ceph config show osd.<OSDID> osd_max_backfills
```

작업이 끝나면 초기 설정으로 되돌려야합니다.

```shell
kubectl rook-ceph ceph tell 'osd.*' injectargs '--osd-max-backfills 1'
```

```shell
kubectl rook-ceph ceph config rm osd osd_mclock_profile
```

:::
