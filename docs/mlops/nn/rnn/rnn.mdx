---
id: rnn
title: Recurrent Neural Network
sidebar_label: RNN
description: Recurrent Neural Network
keywords:
  - Neural Network
  - Recurrent
---

import useBaseUrl from "@docusaurus/useBaseUrl";
import DrwaIOViewer from "@site/src/components/DrawIOViewer";

## RNN(Recurrent Neural Network)

$$
input = Dim(batch\_size, timesteps, input\_size) \\
output = Dim(batch\_size, timesteps, hidden\_size)
$$

<center>
	<figure>
		<DrwaIOViewer src={useBaseUrl("img/mlops/nn/rnn/nn-rnn-simple.drawio")} />
		<figcaption>RNN</figcaption>
	</figure>
</center>

<br />

$$
h_t = \tanh(W_{hx} x_t + W_{hh} h_{t-1} + b_h)
$$

## Vanishing/exploding gradient

긴 시퀀스를 처리하는 RNN은 깊은 네트워크가 되면서($W$가 반복적으로 곱해지면서) `Vanishing/exploding gradient` 문제가 발생하기 쉽습니다.

- Relu와 같이 수렴하지 않는 activation을 사용하면 불안정해질 수 있습니다.
- Exploding gradient 문제가 발견되면, `gradient clipping`을 사용하여 값을 제한 해볼 수 있습니다.

## LSTM(Long Short-Term Memory)

긴 시퀀스를 처리하는 RNN은 순환이 반복되면서 상대적으로 앞쪽 값의 영향이 줄어들 수 있습니다. 이를 `Long-Term Dependency`라고 합니다. LSTM 셀을 사용하면, Long-Term Dependency 문제가 완화되며 훈련 시 빠르게 수렴합니다.

<center>
	<figure>
		<DrwaIOViewer src={useBaseUrl("img/mlops/nn/rnn/nn-rnn-lstm.drawio")} />
		<figcaption>LSTM</figcaption>
	</figure>
</center>

<br />

$$
\begin{aligned}
f_t &= \sigma(W_{xf} x_t + b_{xf} + W_{hf} h_{t-1} + b_{hf}) \\
i_t &= \sigma(W_{xi} x_t + b_{xi} + W_{hi} h_{t-1} + b_{hi}) \\
\widetilde{c_t} &= \tanh(W_{x\tilde{c}} x_t + b_{x\tilde{c}} + W_{h\tilde{c}} h_{t-1} + b_{h\tilde{c}}) \\
o_t &= \sigma(W_{xo} x_t + b_{xo} + W_{ho} h_{t-1} + b_{ho}) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \widetilde{c_t} \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}
$$

- $c_t$는 `cell state`입니다.
- $h_t$는 `output`입니다.
- $f_t$는 `forget gate`로 이전 cell state를 얼마나 잊어버릴 지 결정합니다.
- $i_t$는 `input gate`로 입력 정보를 얼마나 cell state에 저장할 것일지 결정합니다.
- $o_t$는 `output gate`로 업데이트 된 cell state에서 어떤 정보를 내보낼지 결정합니다.

## GRU(Gated Recurrent Unit)

LSTM variants 중 하나입니다.

<center>
	<figure>
		<DrwaIOViewer src={useBaseUrl("img/mlops/nn/rnn/nn-rnn-gru.drawio")} />
		<figcaption>GRU</figcaption>
	</figure>
</center>

<br />

$$
\begin{aligned}
r_t &= \sigma(W_{xr} x_t + b_{xr} + W_{hr} h_{t-1} + b_{hr}) \\
z_t &= \sigma(W_{xz} x_t + b_{xz} + W_{hz} h_{t-1} + b_{hz}) \\
\widetilde{h_t} &= \tanh(W_{x\tilde{h}} x_t + b_{x\tilde{h}} + r_t \odot (W_{h\tilde{h}} h_{t-1} + b_{h\tilde{h}})) \\
h_t &= z_t \odot h_{t-1} + (1 - z_t) \odot \widetilde{h_t}\\
\end{aligned}
$$

- $h_t$는 `outpu`입니다.
- $r_t$는 `reset gate`로 이전 state를 얼마나 output에 포함시킬지 결정합니다.
- $z_t$는 `update gate`로 값이 1에 가까울 수록 이전 state가 저장되고, 0에 가까울 수록 새로운 state가 저장됩니다.
